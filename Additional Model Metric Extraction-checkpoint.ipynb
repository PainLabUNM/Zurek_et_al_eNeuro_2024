{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set_style(\"darkgrid\")\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.decomposition import PCA  \n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "import math\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import aspose.words as aw\n",
    "from scipy.stats import sem\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "from scipy.stats import sem\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d290844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the Dataset\n",
    "\n",
    "hs = pd.read_csv('hDRG_py.csv')\n",
    "hs['multi'] = hs['APs'].apply(lambda x: '0' if x==1 else '1')\n",
    "hs.drop(\"APs\", axis=1, inplace=True)\n",
    "hs.drop(\"mAHP\", axis=1, inplace=True)\n",
    "hs.drop(\"Cell ID\", axis=1, inplace=True)\n",
    "\n",
    "#Intentionally Dropped\n",
    "hs.drop(\"R_slope\", axis=1, inplace=True)\n",
    "hs.drop(\"D_slope\", axis=1, inplace=True)\n",
    "hs.drop(\"Sag\", axis=1, inplace=True)\n",
    "hs.drop(\"Norm_rheo\", axis=1, inplace=True)\n",
    "hs.drop(\"Decay\", axis=1, inplace=True)\n",
    "hs.drop(\"Amp\", axis=1, inplace=True)\n",
    "hs.drop(\"Rise\", axis=1, inplace=True)\n",
    "\n",
    "display(hs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815634b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "labels = hs['multi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.3)\n",
    "\n",
    "X_train.pop('multi')\n",
    "X_test.pop('multi')\n",
    "\n",
    "Y = hs['multi']\n",
    "X = hs.drop(['multi'], axis=1)\n",
    "\n",
    "def standardize(dfs):\n",
    "    mapper = DataFrameMapper([(dfs.columns, StandardScaler())])\n",
    "    scaled_features = mapper.fit_transform(dfs.copy())\n",
    "    dfs = pd.DataFrame(scaled_features, index=dfs.index, columns=dfs.columns)\n",
    "    return dfs\n",
    "\n",
    "hs = standardize(hs)\n",
    "X_train = standardize(X_train)\n",
    "X_test = standardize(X_test)\n",
    "X = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6cbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X) \n",
    "display(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a875d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Feature Selection\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,Y)\n",
    "bfs = SequentialFeatureSelector(lr, k_features='best', forward = False)\n",
    "bfs.fit(X, Y)\n",
    "features = list(bfs.k_feature_names_)\n",
    "print(features)\n",
    "#features = list(map(int, features))\n",
    "lr.fit(X_train[features], y_train)\n",
    "y_predbf = lr.predict(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffda7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Useful Functions\n",
    "\n",
    "def average(arr):\n",
    "    sum = 0\n",
    "    for i in arr:\n",
    "        sum += i\n",
    "    avg = sum/(len(arr))\n",
    "    return avg\n",
    "\n",
    "def median(arr):\n",
    "    arr.sort()\n",
    "    tempArr = []\n",
    "    halfway = len(arr)/2\n",
    "    if (len(arr) % 2 == 0):\n",
    "        for i in range(len(arr)):\n",
    "            if (i == (halfway - 1) or (i == halfway)):\n",
    "                tempArr.append(arr[i])\n",
    "        return (average(tempArr))\n",
    "    elif (len(arr) % 2 == 1):\n",
    "        halfway = math.ceil(halfway)\n",
    "        return arr[halfway]\n",
    "    \n",
    "def rangefunc(arr):\n",
    "    min_val = min(arr)\n",
    "    max_val = max(arr)\n",
    "\n",
    "    return (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf756e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewriting as a function \n",
    "\n",
    "def logisticRegression(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "    \n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    # option 1: solver='liblinear', C=0.23357214690901212, max_iter = 100, penalty='l2'\n",
    "    # option 2: solver='newton-cg', C=0.615848211066026, max_iter = 100, penalty='l2'\n",
    "    # option 3: solver='lbfgs', C=0.615848211066026, max_iter = 100, penalty='l2'\n",
    "    logreg = LogisticRegression(C=0.615848211066026)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    data.append(pd.DataFrame((logreg.coef_), columns = X_train.columns))\n",
    "    predLR = logreg.predict(X_test)\n",
    "    predLR = list(map(int, predLR))\n",
    "    mety_test = list(map(int, y_test))\n",
    "    acc_log = logreg.score(X_test, y_test) \n",
    "    prec_log = precision_score(mety_test, predLR, average='binary')\n",
    "    rec_log = precision_score(mety_test, predLR, average='binary')\n",
    "    F1_log = f1_score(mety_test, predLR, average='binary')\n",
    "    return pd.DataFrame([acc_log, prec_log, rec_log, F1_log]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03af3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "    \n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    knn = KNeighborsClassifier(algorithm='auto', leaf_size=1, metric='minkowski', \n",
    "                           metric_params=None, n_jobs=1, n_neighbors=6, p=1, \n",
    "                           weights='uniform')\n",
    "    knn.fit(X_train, y_train)\n",
    "    predKNN = knn.predict(X_test)\n",
    "    predKNN = list(map(int, predKNN))\n",
    "    mety_test = list(map(int, y_test))\n",
    "    acc_log = (knn.score(X_test, y_test))\n",
    "    prec_log = precision_score(mety_test, predKNN, average='binary')\n",
    "    rec_log = precision_score(mety_test, predKNN, average='binary')\n",
    "    F1_log = f1_score(mety_test, predKNN, average='binary')\n",
    "    return pd.DataFrame([acc_log, prec_log, rec_log, F1_log]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "    \n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    \n",
    "    randForest = RandomForestClassifier(max_depth = 20, criterion = 'gini', min_samples_leaf = 4,\n",
    "                               min_samples_split = 8, n_estimators = 8)\n",
    "    randForest.fit(X_train, y_train)\n",
    "    featdf  = pd.DataFrame(randForest.feature_importances_).transpose()\n",
    "    featdf.columns = X_train.columns\n",
    "    data.append(featdf)\n",
    "    \n",
    "    predRF = randForest.predict(X_test)\n",
    "    predRF = list(map(int, predRF))\n",
    "    mety_test = list(map(int, y_test))\n",
    "    acc_log = (randForest.score(X_test, y_test)) \n",
    "    prec_log = precision_score(mety_test, predRF, average='binary')\n",
    "    rec_log = precision_score(mety_test, predRF, average='binary')\n",
    "    F1_log = f1_score(mety_test, predRF, average='binary')\n",
    "    return pd.DataFrame([acc_log, prec_log, rec_log, F1_log]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b426929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supportVectorClassifier(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "\n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    \n",
    "    svcModel = SVC(C=0.1623776739188721, kernel='linear', max_iter=500)\n",
    "    \n",
    "    svcModel.fit(X_train, y_train)\n",
    "    data.append(pd.DataFrame((svcModel.coef_), columns = X_train.columns))\n",
    "    \n",
    "    predSVC = svcModel.predict(X_test)\n",
    "    predSVC = list(map(int, predSVC))\n",
    "    mety_test = list(map(int, y_test))\n",
    "    acc_log = (svcModel.score(X_test, y_test))\n",
    "    prec_log = precision_score(mety_test, predSVC, average='binary')\n",
    "    rec_log = precision_score(mety_test, predSVC, average='binary')\n",
    "    F1_log = f1_score(mety_test, predSVC, average='binary')\n",
    "    return pd.DataFrame([acc_log, prec_log, rec_log, F1_log]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaee0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "def xgBoost(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2, stratify = labels)\n",
    "\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "\n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    xgbModel = XGBClassifier(booster = 'gblinear', n_estimators=5, learning_rate=1, objective='binary:logistic')\n",
    "    xgbModel.fit(X_train, y_train)\n",
    "    coef = pd.DataFrame(xgbModel.coef_).transpose()\n",
    "    coef.columns = X_train.columns\n",
    "    data.append(coef)\n",
    "    \n",
    "    predXGB = xgbModel.predict(X_test)\n",
    "    predXGB = list(map(int, predXGB))\n",
    "    mety_test = list(map(int, y_test))\n",
    "    y_test = le.fit_transform(y_test)\n",
    "    acc_log = (xgbModel.score(X_test, y_test))\n",
    "    prec_log = precision_score(mety_test, predXGB, average='binary')\n",
    "    rec_log = precision_score(mety_test, predXGB, average='binary')\n",
    "    F1_log = f1_score(mety_test, predXGB, average='binary')\n",
    "    return pd.DataFrame([acc_log, prec_log, rec_log, F1_log]).transpose() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29048cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "num_simulations = 100\n",
    "startTime = 0 \n",
    "endTime = 0\n",
    "data = []\n",
    "acc = []\n",
    "df = pd.DataFrame(columns = X_train.columns)\n",
    "metricdf = pd.DataFrame(columns = ['Acc', 'Pre', 'Rec', 'F1'])\n",
    "outliers = pd.DataFrame(columns = X_test.columns)\n",
    "\n",
    "\n",
    "# Loop through simulations\n",
    "startTime = time.perf_counter()\n",
    "for i in range(num_simulations):\n",
    "    stats.append(logisticRegression(data))\n",
    "    \n",
    "endTime = time.perf_counter()\n",
    "metricdf = pd.concat(stats, ignore_index=True)\n",
    "metricdf.columns = ['Acc', 'Pre', 'Rec', 'F1']\n",
    "metricdf.loc['mean'] = metricdf.mean()\n",
    "metricdf.loc['sem'] = metricdf.sem()\n",
    "display(metricdf)\n",
    "    \n",
    "sns.histplot(data=metricdf['Acc'], binwidth=0.02)\n",
    "plt.title('Accuracy Hist')\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Measure the runtime of a simulation\n",
    "\n",
    "runTime = endTime - startTime\n",
    "print(\"Duration: \", runTime, \"s\")\n",
    "\n",
    "# Coefficient Data\n",
    "\n",
    "df = pd.concat(data, ignore_index=True)\n",
    "df.loc['mean'] = df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing with Monte Carlo Simulations\n",
    "\n",
    "statsLR = []\n",
    "statsKNN = []\n",
    "statsRF = []\n",
    "statsSVC = []\n",
    "statsXGB = []\n",
    "\n",
    "dfLR = pd.DataFrame()\n",
    "dfKNN = pd.DataFrame()\n",
    "dfRF = pd.DataFrame()\n",
    "dfSVC = pd.DataFrame()\n",
    "dfXGB = pd.DataFrame()\n",
    "\n",
    "num_simulations = 1000\n",
    "startTime = 0 \n",
    "endTime = 0\n",
    "dataLR = []\n",
    "dataRF = []\n",
    "dataSVC = []\n",
    "dataXGB = []\n",
    "df = pd.DataFrame(columns = X_train.columns)\n",
    "outliers = pd.DataFrame(columns = X_test.columns)\n",
    "\n",
    "\n",
    "# Loop through simulations\n",
    "startTime = time.perf_counter()\n",
    "for i in range(num_simulations):\n",
    "    statsLR.append(logisticRegression(dataLR))\n",
    "    \n",
    "for i in range(num_simulations):\n",
    "    statsKNN.append(KNN())\n",
    "    \n",
    "for i in range(num_simulations):\n",
    "    statsRF.append(randomForest(dataRF))\n",
    "    \n",
    "for i in range(num_simulations):\n",
    "    statsSVC.append(supportVectorClassifier(dataSVC))\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    statsXGB.append(xgBoost(dataXGB))\n",
    "    \n",
    "endTime = time.perf_counter()\n",
    "    \n",
    "#sns.histplot(data=stats, binwidth=0.5)\n",
    "\n",
    "models = ['Logistic Regression', 'K Nearest Neighbors', 'Random Forest',\n",
    "         'Support Vector Classifier', 'eXtreme Gradient Boost']\n",
    "stat_lst = [statsLR, statsKNN, statsRF, statsSVC, statsXGB]\n",
    "df_lst = [dfLR, dfKNN, dfRF, dfSVC, dfXGB]\n",
    "\n",
    "# Iterate through the five models\n",
    "for i in range(len(models)):\n",
    "    \n",
    "    df_lst[i] = pd.concat(stat_lst[i], ignore_index=True)\n",
    "    df_lst[i].columns = ['Acc', 'Pre', 'Rec', 'F1']\n",
    "    df_lst[i].loc['mean'] = df_lst[i].mean()\n",
    "    df_lst[i].loc['sem'] = df_lst[i].sem()\n",
    "    \n",
    "    # Draw the density plot\n",
    "    sns.distplot(df_lst[i]['Acc'], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = models[i])\n",
    "    \n",
    "    display(df_lst[i])\n",
    "    \n",
    "# Plotting Modifications\n",
    "plt.legend(prop={'size': 8}, title = 'Model')\n",
    "plt.title('Density Plot with Multiple Models')\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('Accuracy.tif')\n",
    "\n",
    "# Measure the runtime of a simulation\n",
    "\n",
    "runTime = endTime - startTime\n",
    "print(\"Duration: \", runTime, \"s\")\n",
    "\n",
    "#Coefficient Plots\n",
    "dfLR = pd.concat(dataLR, ignore_index=True)\n",
    "dfLR.loc['mean'] = dfLR.mean()\n",
    "\n",
    "dfRF = pd.concat(dataRF, ignore_index=True)\n",
    "dfRF.loc['mean'] = dfRF.mean()\n",
    "\n",
    "dfSVC = pd.concat(dataSVC, ignore_index=True)\n",
    "dfSVC.loc['mean'] = dfSVC.mean()\n",
    "\n",
    "dfXGB = pd.concat(dataXGB, ignore_index=True)\n",
    "dfXGB.loc['mean'] = dfXGB.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c794d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above Ordered and ABS\n",
    "\n",
    "dfLR2 = dfLR.loc['mean'].to_frame()\n",
    "dfLR2 = abs(dfLR2)\n",
    "dfLR2 = dfLR2.sort_values(by = ['mean'], ascending=False).transpose().reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Logistic Regression Coefficient Weights')\n",
    "bar_plot = plt.bar(range(len(dfLR2.columns)), dfLR2.loc[0])\n",
    "plt.xticks(range(len(dfLR2.columns)), dfLR2.columns, fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(fontsize=12)\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "plt.savefig('LRCoefS2.png')    \n",
    "plt.show()\n",
    "display(dfLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713260c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordered\n",
    "\n",
    "dfRF2 = dfRF.loc['mean'].to_frame()\n",
    "dfRF2 = dfRF2.sort_values(by = ['mean'], ascending=False).transpose().reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Random Forest Feature Importance')\n",
    "bar_plot = plt.bar(range(len(dfRF2.columns)), dfRF2.loc[0])\n",
    "plt.xticks(range(len(dfRF2.columns)), dfRF2.columns, fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(fontsize=12)\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "plt.savefig('RFCoefS2.png')    \n",
    "plt.show()\n",
    "display(dfRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordered and ABS\n",
    "\n",
    "dfSVC2 = dfSVC.loc['mean'].to_frame()\n",
    "dfSVC2 = abs(dfSVC2)\n",
    "dfSVC2 = dfSVC2.sort_values(by = ['mean'], ascending=False).transpose().reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('SVC Coefficient Weights')\n",
    "bar_plot = plt.bar(range(len(dfSVC2.columns)), dfSVC2.loc[0])\n",
    "plt.xticks(range(len(dfSVC2.columns)), dfSVC2.columns, fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(fontsize=12)\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=10) \n",
    "plt.savefig('SVCCoefS2.jpg')\n",
    "plt.show()\n",
    "display(dfSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e686e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Average Rank of Features: \n",
    "\n",
    "rank_list = [dfLR2, dfRF2, dfSVC2] #list of coefficient frames\n",
    "for df in rank_list:\n",
    "    df.loc[len(df.index)] = list(range(1,15)) # adding row in df representing ranking\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining rankings across models and then displaying average ranking\n",
    "\n",
    "for feature in X.columns.to_list():\n",
    "    total_rank = 0;\n",
    "    for df in rank_list: \n",
    "        total_rank += df.iloc[1][feature]\n",
    "    total_rank = total_rank / len(rank_list)\n",
    "    print(feature ,\": \", total_rank)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fd797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
