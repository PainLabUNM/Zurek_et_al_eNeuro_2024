{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set_style(\"darkgrid\")\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.decomposition import PCA  \n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "import math\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import aspose.words as aw\n",
    "from scipy.stats import sem\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d290844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the Dataset\n",
    "\n",
    "hs = pd.read_csv('hDRG_py.csv')\n",
    "hs['multi'] = hs['APs'].apply(lambda x: '0' if x==1 else '1')\n",
    "hs.drop(\"APs\", axis=1, inplace=True)\n",
    "hs.drop(\"mAHP\", axis=1, inplace=True)\n",
    "hs.drop(\"Cell ID\", axis=1, inplace=True)\n",
    "\n",
    "#Intentionally Dropped\n",
    "hs.drop(\"R_slope\", axis=1, inplace=True)\n",
    "hs.drop(\"D_slope\", axis=1, inplace=True)\n",
    "hs.drop(\"Sag\", axis=1, inplace=True)\n",
    "hs.drop(\"Norm_rheo\", axis=1, inplace=True)\n",
    "hs.drop(\"Decay\", axis=1, inplace=True)\n",
    "hs.drop(\"Amp\", axis=1, inplace=True)\n",
    "hs.drop(\"Rise\", axis=1, inplace=True)\n",
    "\n",
    "display(hs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815634b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "labels = hs['multi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.3)\n",
    "\n",
    "X_train.pop('multi')\n",
    "X_test.pop('multi')\n",
    "\n",
    "Y = hs['multi']\n",
    "X = hs.drop(['multi'], axis=1)\n",
    "\n",
    "def standardize(dfs):\n",
    "    mapper = DataFrameMapper([(dfs.columns, StandardScaler())])\n",
    "    scaled_features = mapper.fit_transform(dfs.copy())\n",
    "    dfs = pd.DataFrame(scaled_features, index=dfs.index, columns=dfs.columns)\n",
    "    return dfs\n",
    "\n",
    "hs = standardize(hs)\n",
    "X_train = standardize(X_train)\n",
    "X_test = standardize(X_test)\n",
    "X = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6cbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X) \n",
    "display(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims = 100\n",
    "\n",
    "shap_values_per_cv = dict()\n",
    "for sample in X.index:\n",
    "    ## Create keys for each sample\n",
    "    shap_values_per_cv[sample] = {} \n",
    "    ## Then, keys for each CV fold within each sample\n",
    "    for num_sim in range(num_sims):\n",
    "        shap_values_per_cv[sample][num_sim] = {}\n",
    "\n",
    "for i, num_sim in enumerate(range(num_sims)):\n",
    "    #Establish CV scheme\n",
    "    ix_training, ix_test, iy_training, iy_test = [], [], [], []\n",
    "    for i in range(num_sim):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "    \n",
    "        # Standardize the data\n",
    "        X_train = standardize(X_train)\n",
    "        X_test = standardize(X_test)\n",
    "        X_train.pop('multi')\n",
    "        X_test.pop('multi')\n",
    "    \n",
    "        # Append the standardized splits to the lists\n",
    "        ix_training.append(X_train)\n",
    "        ix_test.append(X_test)\n",
    "        iy_training.append(y_train)\n",
    "        iy_test.append(y_test)\n",
    "        \n",
    "# Initialize the SHAP values storage dictionary\n",
    "total_shap_values = []\n",
    "\n",
    "# Second loop: fit the model and generate SHAP values for each split\n",
    "for num_sim in range(num_sims - 1):\n",
    "    model = RandomForestClassifier(max_depth = 20, criterion = 'gini', min_samples_leaf = 4,\n",
    "                               min_samples_split = 8, n_estimators = 8)\n",
    "    model.fit(ix_training[num_sim], iy_training[num_sim])\n",
    "    yhat = model.predict(ix_test[num_sim])\n",
    "    result = mean_squared_error(iy_test[num_sim], yhat)\n",
    "    print('RMSE for split', num_sim, ':', round(np.sqrt(result), 4))\n",
    "    \n",
    "    # Use SHAP to explain predictions\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values_bar = explainer(ix_test[num_sim])\n",
    "    shap_values = explainer.shap_values(ix_test[num_sim])\n",
    "    \n",
    "    for values in shap_values: \n",
    "        for value in values:\n",
    "            total_shap_values.append(value)\n",
    "        \n",
    "plt.title(\"RF SHAP\", fontsize=14)\n",
    "shap.summary_plot(np.array(total_shap_values), X.columns, show=False)\n",
    "plt.savefig(\"RFSHAP.jpg\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"RF SHAP Bar\", fontsize=12)\n",
    "shap.summary_plot(np.array(total_shap_values), ix_test[0].columns, plot_type=\"bar\", show=False)\n",
    "plt.savefig(\"SHAPBarPlot.jpg\")\n",
    "plt.savefig('HLRSHAPBAR.jpg')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims = 100\n",
    "\n",
    "shap_values_per_cv = dict()\n",
    "for sample in X.index:\n",
    "    ## Create keys for each sample\n",
    "    shap_values_per_cv[sample] = {} \n",
    "    ## Then, keys for each CV fold within each sample\n",
    "    for num_sim in range(num_sims):\n",
    "        shap_values_per_cv[sample][num_sim] = {}\n",
    "\n",
    "for i, num_sim in enumerate(range(num_sims)):\n",
    "    #Establish CV scheme\n",
    "    ix_training, ix_test, iy_training, iy_test = [], [], [], []\n",
    "    for i in range(num_sim):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "    \n",
    "        # Standardize the data\n",
    "        X_train = standardize(X_train)\n",
    "        X_test = standardize(X_test)\n",
    "        X_train.pop('multi')\n",
    "        X_test.pop('multi')\n",
    "    \n",
    "        # Append the standardized splits to the lists\n",
    "        ix_training.append(X_train)\n",
    "        ix_test.append(X_test)\n",
    "        iy_training.append(y_train)\n",
    "        iy_test.append(y_test)\n",
    "        \n",
    "# Initialize the SHAP values storage dictionary\n",
    "total_shap_values = []\n",
    "\n",
    "# Second loop: fit the model and generate SHAP values for each split\n",
    "for num_sim in range(num_sims - 1):\n",
    "    model = LogisticRegression(C=0.615848211066026)\n",
    "    model.fit(ix_training[num_sim], iy_training[num_sim])\n",
    "    yhat = model.predict(ix_test[num_sim])\n",
    "    result = mean_squared_error(iy_test[num_sim], yhat)\n",
    "    print('RMSE for split', num_sim, ':', round(np.sqrt(result), 4))\n",
    "    \n",
    "    # Use SHAP to explain predictions\n",
    "    explainer = shap.Explainer(model, ix_test[num_sim])\n",
    "    shap_values = explainer.shap_values(ix_test[num_sim])\n",
    "    \n",
    "    for values in shap_values: \n",
    "            total_shap_values.append(values)\n",
    "        \n",
    "plt.title(\"LR SHAP\", fontsize=14)\n",
    "shap.summary_plot(np.array(total_shap_values), X.columns, show=False)\n",
    "plt.savefig(\"LRSHAP.jpg\")\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims = 100\n",
    "\n",
    "shap_values_per_cv = dict()\n",
    "for sample in X.index:\n",
    "    ## Create keys for each sample\n",
    "    shap_values_per_cv[sample] = {} \n",
    "    ## Then, keys for each CV fold within each sample\n",
    "    for num_sim in range(num_sims):\n",
    "        shap_values_per_cv[sample][num_sim] = {}\n",
    "\n",
    "for i, num_sim in enumerate(range(num_sims)):\n",
    "    #Establish CV scheme\n",
    "    ix_training, ix_test, iy_training, iy_test = [], [], [], []\n",
    "    for i in range(num_sim):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "    \n",
    "        # Standardize the data\n",
    "        X_train = standardize(X_train)\n",
    "        X_test = standardize(X_test)\n",
    "        X_train.pop('multi')\n",
    "        X_test.pop('multi')\n",
    "    \n",
    "        # Append the standardized splits to the lists\n",
    "        ix_training.append(X_train)\n",
    "        ix_test.append(X_test)\n",
    "        iy_training.append(y_train)\n",
    "        iy_test.append(y_test)\n",
    "        \n",
    "# Initialize the SHAP values storage dictionary\n",
    "total_shap_values = []\n",
    "\n",
    "# Second loop: fit the model and generate SHAP values for each split\n",
    "for num_sim in range(num_sims - 1):\n",
    "    model = KNeighborsClassifier(algorithm='auto', leaf_size=1, metric='minkowski', \n",
    "                           metric_params=None, n_jobs=1, n_neighbors=6, p=1, \n",
    "                           weights='uniform')\n",
    "    model.fit(ix_training[num_sim], iy_training[num_sim])\n",
    "    yhat = model.predict(ix_test[num_sim])\n",
    "    result = mean_squared_error(iy_test[num_sim], yhat)\n",
    "    print('RMSE for split', num_sim, ':', round(np.sqrt(result), 4))\n",
    "    \n",
    "    # Use SHAP to explain predictions\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, ix_test[num_sim])\n",
    "    shap_values = explainer.shap_values(ix_test[num_sim])\n",
    "    \n",
    "    \n",
    "    for values in shap_values: \n",
    "        for value in values:\n",
    "            total_shap_values.append(value)\n",
    "            \n",
    "plt.title(\"KNN SHAP\", fontsize=14)\n",
    "shap.summary_plot(np.array(total_shap_values), X.columns, show=False)\n",
    "plt.savefig(\"KNNSHAP.jpg\")\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab49ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims = 100\n",
    "\n",
    "shap_values_per_cv = dict()\n",
    "for sample in X.index:\n",
    "    ## Create keys for each sample\n",
    "    shap_values_per_cv[sample] = {} \n",
    "    ## Then, keys for each CV fold within each sample\n",
    "    for num_sim in range(num_sims):\n",
    "        shap_values_per_cv[sample][num_sim] = {}\n",
    "\n",
    "for i, num_sim in enumerate(range(num_sims)):\n",
    "    #Verbose \n",
    "    print('\\n------------ CV Repeat number:', num_sim)\n",
    "    #Establish CV scheme\n",
    "    ix_training, ix_test, iy_training, iy_test = [], [], [], []\n",
    "    for i in range(num_sim):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "    \n",
    "        # Standardize the data\n",
    "        X_train = standardize(X_train)\n",
    "        X_test = standardize(X_test)\n",
    "        X_train.pop('multi')\n",
    "        X_test.pop('multi')\n",
    "    \n",
    "        # Append the standardized splits to the lists\n",
    "        ix_training.append(X_train)\n",
    "        ix_test.append(X_test)\n",
    "        iy_training.append(y_train)\n",
    "        iy_test.append(y_test)\n",
    "        \n",
    "print(len(ix_training))\n",
    "# Initialize the SHAP values storage dictionary\n",
    "total_shap_values = []\n",
    "\n",
    "# Second loop: fit the model and generate SHAP values for each split\n",
    "for num_sim in range(num_sims - 1):\n",
    "    model = SVC(C=0.1623776739188721, kernel='linear', max_iter=500)\n",
    "    model.fit(ix_training[num_sim], iy_training[num_sim])\n",
    "    yhat = model.predict(ix_test[num_sim])\n",
    "    result = mean_squared_error(iy_test[num_sim], yhat)\n",
    "    print('RMSE for split', num_sim, ':', round(np.sqrt(result), 4))\n",
    "    \n",
    "    # Use SHAP to explain predictions\n",
    "    explainer = shap.Explainer(model, ix_test[num_sim])\n",
    "    shap_values = explainer.shap_values(ix_test[num_sim])\n",
    "    \n",
    "    \n",
    "    for value in shap_values:\n",
    "        total_shap_values.append(value)\n",
    "    \n",
    "plt.title(\"SVC SHAP\", fontsize=14)\n",
    "shap.summary_plot(np.array(total_shap_values), X.columns, show=False)\n",
    "plt.savefig(\"SVCSHAP.jpg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims = 100\n",
    "\n",
    "shap_values_per_cv = dict()\n",
    "for sample in X.index:\n",
    "    ## Create keys for each sample\n",
    "    shap_values_per_cv[sample] = {} \n",
    "    ## Then, keys for each CV fold within each sample\n",
    "    for num_sim in range(num_sims):\n",
    "        shap_values_per_cv[sample][num_sim] = {}\n",
    "\n",
    "for i, num_sim in enumerate(range(num_sims)):\n",
    "    #Verbose \n",
    "    print('\\n------------ CV Repeat number:', num_sim)\n",
    "    #Establish CV scheme\n",
    "    ix_training, ix_test, iy_training, iy_test = [], [], [], []\n",
    "    for i in range(num_sim):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "        \n",
    "        y_train = le.fit_transform(y_train)\n",
    "    \n",
    "        # Standardize the data\n",
    "        X_train = standardize(X_train)\n",
    "        X_test = standardize(X_test)\n",
    "        X_train.pop('multi')\n",
    "        X_test.pop('multi')\n",
    "    \n",
    "        # Append the standardized splits to the lists\n",
    "        ix_training.append(X_train)\n",
    "        ix_test.append(X_test)\n",
    "        iy_training.append(y_train)\n",
    "        iy_test.append(y_test)\n",
    "        \n",
    "print(len(ix_training))\n",
    "# Initialize the SHAP values storage dictionary\n",
    "total_shap_values = []\n",
    "\n",
    "# Second loop: fit the model and generate SHAP values for each split\n",
    "for num_sim in range(num_sims - 1):\n",
    "    model = XGBClassifier(booster = 'gblinear', n_estimators=2, learning_rate=1, objective='binary:logistic')\n",
    "    model.fit(ix_training[num_sim], iy_training[num_sim])\n",
    "    yhat = model.predict(ix_test[num_sim])\n",
    "    result = mean_squared_error(iy_test[num_sim], yhat)\n",
    "    print('RMSE for split', num_sim, ':', round(np.sqrt(result), 4))\n",
    "    \n",
    "    # Use SHAP to explain predictions\n",
    "    explainer = shap.Explainer(model, ix_test[num_sim])\n",
    "    shap_values = explainer.shap_values(ix_test[num_sim])\n",
    "    \n",
    "    \n",
    "    for value in shap_values:\n",
    "        total_shap_values.append(value)\n",
    "    \n",
    "plt.title(\"XGB SHAP\", fontsize=14)\n",
    "shap.summary_plot(np.array(total_shap_values), X.columns, show=False)\n",
    "plt.savefig(\"XGBSHAP.jpg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffda7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Useful Functions\n",
    "\n",
    "def average(arr):\n",
    "    sum = 0\n",
    "    for i in arr:\n",
    "        sum += i\n",
    "    avg = sum/(len(arr))\n",
    "    return avg\n",
    "\n",
    "def median(arr):\n",
    "    arr.sort()\n",
    "    tempArr = []\n",
    "    halfway = len(arr)/2\n",
    "    if (len(arr) % 2 == 0):\n",
    "        for i in range(len(arr)):\n",
    "            if (i == (halfway - 1) or (i == halfway)):\n",
    "                tempArr.append(arr[i])\n",
    "        return (average(tempArr))\n",
    "    elif (len(arr) % 2 == 1):\n",
    "        halfway = math.ceil(halfway)\n",
    "        return arr[halfway]\n",
    "    \n",
    "def rangefunc(arr):\n",
    "    min_val = min(arr)\n",
    "    max_val = max(arr)\n",
    "\n",
    "    return (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bcd9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
