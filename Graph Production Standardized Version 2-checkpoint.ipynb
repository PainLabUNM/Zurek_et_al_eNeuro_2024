{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set_style(\"darkgrid\")\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.decomposition import PCA  \n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "import math\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import aspose.words as aw\n",
    "from scipy.stats import sem\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d290844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the Dataset\n",
    "\n",
    "hs = pd.read_csv('hDRG_py.csv')\n",
    "hs['multi'] = hs['APs'].apply(lambda x: '0' if x==1 else '1')\n",
    "hs.drop(\"APs\", axis=1, inplace=True)\n",
    "hs.drop(\"mAHP\", axis=1, inplace=True)\n",
    "hs.drop(\"Cell ID\", axis=1, inplace=True)\n",
    "\n",
    "#Intentionally Dropped\n",
    "hs.drop(\"R_slope\", axis=1, inplace=True)\n",
    "hs.drop(\"D_slope\", axis=1, inplace=True)\n",
    "hs.drop(\"Sag\", axis=1, inplace=True)\n",
    "hs.drop(\"Norm_rheo\", axis=1, inplace=True)\n",
    "hs.drop(\"Decay\", axis=1, inplace=True)\n",
    "hs.drop(\"Amp\", axis=1, inplace=True)\n",
    "hs.drop(\"Rise\", axis=1, inplace=True)\n",
    "\n",
    "display(hs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815634b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "labels = hs['multi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.3)\n",
    "\n",
    "X_train.pop('multi')\n",
    "X_test.pop('multi')\n",
    "\n",
    "Y = hs['multi']\n",
    "X = hs.drop(['multi'], axis=1)\n",
    "\n",
    "def standardize(dfs):\n",
    "    mapper = DataFrameMapper([(dfs.columns, StandardScaler())])\n",
    "    scaled_features = mapper.fit_transform(dfs.copy())\n",
    "    dfs = pd.DataFrame(scaled_features, index=dfs.index, columns=dfs.columns)\n",
    "    return dfs\n",
    "\n",
    "hs = standardize(hs)\n",
    "X_train = standardize(X_train)\n",
    "X_test = standardize(X_test)\n",
    "X = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6cbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X) \n",
    "display(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a875d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Feature Selection\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,Y)\n",
    "bfs = SequentialFeatureSelector(lr, k_features='best', forward = False)\n",
    "bfs.fit(X, Y)\n",
    "features = list(bfs.k_feature_names_)\n",
    "print(features)\n",
    "#features = list(map(int, features))\n",
    "lr.fit(X_train[features], y_train)\n",
    "y_predbf = lr.predict(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffda7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Useful Functions\n",
    "\n",
    "def average(arr):\n",
    "    sum = 0\n",
    "    for i in arr:\n",
    "        sum += i\n",
    "    avg = sum/(len(arr))\n",
    "    return avg\n",
    "\n",
    "def median(arr):\n",
    "    arr.sort()\n",
    "    tempArr = []\n",
    "    halfway = len(arr)/2\n",
    "    if (len(arr) % 2 == 0):\n",
    "        for i in range(len(arr)):\n",
    "            if (i == (halfway - 1) or (i == halfway)):\n",
    "                tempArr.append(arr[i])\n",
    "        return (average(tempArr))\n",
    "    elif (len(arr) % 2 == 1):\n",
    "        halfway = math.ceil(halfway)\n",
    "        return arr[halfway]\n",
    "    \n",
    "def rangefunc(arr):\n",
    "    min_val = min(arr)\n",
    "    max_val = max(arr)\n",
    "\n",
    "    return (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7375c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR Optimization & Hyperparameterization\n",
    "\n",
    "logrego = LogisticRegression()\n",
    "\n",
    "param_grid = [    \n",
    "    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C' : np.logspace(-4, 4, 20),\n",
    "    'solver' : ['lbfgs','newton-cg', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter' : [100, 1000,2500, 5000]\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(logrego, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "best_clf = clf.fit(X,Y)\n",
    "print(best_clf.best_estimator_)\n",
    "print('Mean Cross-Validated Score: ',best_clf.best_score_)\n",
    "print('Best Parameters',best_clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eca212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf756e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewriting as a function \n",
    "\n",
    "def logisticRegression(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "    \n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    # option 1: solver='liblinear', C=0.23357214690901212, max_iter = 100, penalty='l2'\n",
    "    # option 2: solver='newton-cg', C=0.615848211066026, max_iter = 100, penalty='l2'\n",
    "    # option 3: solver='lbfgs', C=0.615848211066026, max_iter = 100, penalty='l2'\n",
    "    logreg = LogisticRegression(C=0.615848211066026)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    data.append(pd.DataFrame((logreg.coef_), columns = X_train.columns))\n",
    "    acc_log = round(logreg.score(X_test, y_test) * 100, 2)\n",
    "                \n",
    "    return acc_log          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96328fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "knearest = KNeighborsClassifier()\n",
    "\n",
    "param_grid = [    \n",
    "    {'leaf_size' : list(range(1,50)),\n",
    "     'n_neighbors' : list(range(1,30)),\n",
    "     'p' : [1,2]\n",
    "    }\n",
    "]\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knearest, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "best_clf = clf.fit(X,Y)\n",
    "best_clf.best_estimator_\n",
    "print('Mean Cross-Validated Score: ',best_clf.best_score_)\n",
    "print('Best Parameters',best_clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03af3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "    \n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    knn = KNeighborsClassifier(algorithm='auto', leaf_size=1, metric='minkowski', \n",
    "                           metric_params=None, n_jobs=1, n_neighbors=6, p=1, \n",
    "                           weights='uniform')\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_predictions = knn.predict(X_test)\n",
    "    acc_knn = round(knn.score(X_test, y_test) * 100, 2)\n",
    "    return acc_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running/Optimizing the Random Forest\n",
    "\n",
    "RF = RandomForestClassifier()\n",
    "\n",
    "# Create param-grid to perform grid-search on\n",
    "\n",
    "param_grid = [    \n",
    "    {'n_estimators' : [4, 8, 12, 16, 22],\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_depth' : [10, 20, 30, 40, 50],\n",
    "    'min_samples_split' : [2, 4, 6, 8, 12, 16],\n",
    "    'min_samples_leaf' : [2, 4, 8, 10, 12, 16]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Fit your model using gridsearch\n",
    "model = GridSearchCV(RF, param_grid, cv=5, verbose=1)\n",
    "best_model = model.fit(X, Y)\n",
    "best_model.best_estimator_\n",
    "print('Mean Cross-Validated Score: ',best_model.best_score_)\n",
    "print('Best Parameters',best_model.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "    \n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    \n",
    "    randForest = RandomForestClassifier(max_depth = 20, criterion = 'gini', min_samples_leaf = 4,\n",
    "                               min_samples_split = 8, n_estimators = 8)\n",
    "    randForest.fit(X_train, y_train)\n",
    "    featdf  = pd.DataFrame(randForest.feature_importances_).transpose()\n",
    "    featdf.columns = X_train.columns\n",
    "    data.append(featdf)\n",
    "    \n",
    "\n",
    "    acc_log = round(randForest.score(X_test, y_test) * 100, 2)\n",
    "                \n",
    "    return acc_log        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validated Score:  0.8730994152046785\n",
      "Best Parameters {'C': 0.1623776739188721, 'kernel': 'linear', 'max_iter': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherw\\Python\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sherw\\Python\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sherw\\Python\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "supportVC = SVC()\n",
    "\n",
    "# Create param-grid to perform grid-search on\n",
    "\n",
    "param_grid = [    \n",
    "    {\n",
    "    'kernel' : ['linear'],\n",
    "    'C' : np.logspace(-3, 3, 20),\n",
    "    'max_iter' : [100, 1000,2500, 5000]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Fit your model using gridsearch\n",
    "model = GridSearchCV(supportVC, param_grid, cv=5, verbose=1)\n",
    "best_model = model.fit(X, Y)\n",
    "best_model.best_estimator_\n",
    "print('Mean Cross-Validated Score: ',best_model.best_score_)\n",
    "print('Best Parameters',best_model.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b426929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supportVectorClassifier(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "\n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    \n",
    "    svcModel = SVC(C=0.1623776739188721, kernel='linear', max_iter=500)\n",
    "    \n",
    "    svcModel.fit(X_train, y_train)\n",
    "    data.append(pd.DataFrame((svcModel.coef_), columns = X_train.columns))\n",
    "\n",
    "    acc_log = round(svcModel.score(X_test, y_test) * 100, 2)\n",
    "                \n",
    "    return acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a056dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extremeGB = XGBClassifier()\n",
    "\n",
    "# Create param-grid to perform grid-search on\n",
    "\n",
    "param_grid = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "# Fit your model using gridsearch\n",
    "model = GridSearchCV(extremeGB, param_grid, cv=5, verbose=1)\n",
    "Y = le.fit_transform(Y)\n",
    "best_model = model.fit(X, Y)\n",
    "best_model.best_estimator_\n",
    "print('Mean Cross-Validated Score: ',best_model.best_score_)\n",
    "print('Best Parameters',best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b3731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extremeGB = XGBClassifier()\n",
    "\n",
    "# Create param-grid to perform grid-search on\n",
    "\n",
    "param_grid = {\n",
    "        'max_depth': [1, 2, 3, 4],\n",
    "        'gamma': [1, 0.75],\n",
    "        'reg_alpha' : np.linspace(20, 80, 4),\n",
    "        'reg_lambda' : np.linspace(0, 1, 4),\n",
    "        'colsample_bytree' : np.linspace(0.4, 0.7, 4),\n",
    "        'min_child_weight' : np.linspace(0, 1, 4),\n",
    "        }\n",
    "\n",
    "# Fit your model using gridsearch\n",
    "model = GridSearchCV(extremeGB, param_grid, cv=5, verbose=1)\n",
    "Y = le.fit_transform(Y)\n",
    "best_model = model.fit(X, Y)\n",
    "best_model.best_estimator_\n",
    "print('Mean Cross-Validated Score: ',best_model.best_score_)\n",
    "print('Best Parameters',best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8438713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2)\n",
    "\n",
    "X_train.pop('multi')\n",
    "X_test.pop('multi')\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab43064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 10,\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    clf=XGBClassifier(\n",
    "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree']))\n",
    "    \n",
    "    evaluation = [(X_train, y_train), (X_test, y_test)]\n",
    "    \n",
    "    clf.fit(X_train, y_train,\n",
    "            eval_set=evaluation, eval_metric=\"acc\",\n",
    "            early_stopping_rounds=20,verbose=False)\n",
    "    \n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred>0.5)\n",
    "    print (\"SCORE:\", accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)\n",
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852c3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Type Booster\n",
    "\n",
    "def xgBoost():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2, stratify = labels)\n",
    "\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "\n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    xgb = XGBClassifier(colsample_bytree = 0.8938665087724189, gamma = 5.7202267126164825, \n",
    "                        max_depth = 7, min_child_weight = 1.0) #reg_alpha = 130.0, reg_lambda = 0.3992325689441024)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    preds = xgb.predict(X_test)\n",
    "    acc = (accuracy_score(y_test.astype(int), preds) * 100.00) \n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Booster\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "def xgBoost(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hs, labels, test_size=0.2, stratify = labels)\n",
    "\n",
    "    X_train.pop('multi')\n",
    "    X_test.pop('multi')\n",
    "\n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    Y = hs['multi']\n",
    "    X = hs.drop(['multi'], axis=1)\n",
    "    \n",
    "    xgb = XGBClassifier(booster = 'gblinear', n_estimators=5, learning_rate=1, objective='binary:logistic')\n",
    "    xgb.fit(X_train, y_train)\n",
    "    coef = pd.DataFrame(xgb.coef_).transpose()\n",
    "    coef.columns = X_train.columns\n",
    "    data.append(coef)\n",
    "    \n",
    "    preds = xgb.predict(X_test)\n",
    "    acc = (accuracy_score(y_test.astype(int), preds) * 100.00) \n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a324e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyzing with Monte Carlo Simulations\n",
    "\n",
    "statsLR = []\n",
    "statsKNN = []\n",
    "statsRF = []\n",
    "statsSVC = []\n",
    "statsXGB = []\n",
    "num_simulations = 1000\n",
    "startTime = 0 \n",
    "endTime = 0\n",
    "dataLR = []\n",
    "dataRF = []\n",
    "dataSVC = []\n",
    "dataXGB = []\n",
    "df = pd.DataFrame(columns = X_train.columns)\n",
    "outliers = pd.DataFrame(columns = X_test.columns)\n",
    "\n",
    "\n",
    "# Loop through simulations\n",
    "startTime = time.perf_counter()\n",
    "for i in range(num_simulations):\n",
    "    statsLR.append(logisticRegression(dataLR))\n",
    "    \n",
    "for i in range(num_simulations):\n",
    "    statsKNN.append(KNN())\n",
    "    \n",
    "for i in range(num_simulations):\n",
    "    statsRF.append(randomForest(dataRF))\n",
    "    \n",
    "for i in range(num_simulations):\n",
    "    statsSVC.append(supportVectorClassifier(dataSVC))\n",
    "\n",
    "# Change function depending on use of linear or tree\n",
    "for i in range(num_simulations):\n",
    "    statsXGB.append(xgBoost(dataXGB))\n",
    "    \n",
    "endTime = time.perf_counter()\n",
    "    \n",
    "#sns.histplot(data=stats, binwidth=0.5)\n",
    "\n",
    "models = ['Logistic Regression', 'K Nearest Neighbors', 'Random Forest',\n",
    "         'Support Vector Classifier', \"eXtreme Gradient Boost\"]\n",
    "stat_lst = [statsLR, statsKNN, statsRF, statsSVC, statsXGB]\n",
    "\n",
    "# Iterate through the five models\n",
    "for i in range(len(models)):\n",
    "    # Draw the density plot\n",
    "    sns.distplot(stat_lst[i], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = models[i])\n",
    "    \n",
    "# Plotting Modifications\n",
    "plt.legend(prop={'size': 8}, title = 'Model')\n",
    "plt.title('Density Plot with Multiple Models')\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('Accuracy.tif')\n",
    "\n",
    "# Provide metrics for the simulations\n",
    "\n",
    "print(\"The average is: \", average(statsLR))\n",
    "print(\"The median is: \", median(statsLR))\n",
    "\n",
    "print(\"The average is: \", average(statsKNN))\n",
    "print(\"The median is: \", median(statsKNN))\n",
    "\n",
    "print(\"The average is: \", average(statsRF))\n",
    "print(\"The median is: \", median(statsRF))\n",
    "\n",
    "print(\"The average is: \", average(statsSVC))\n",
    "print(\"The median is: \", median(statsSVC))\n",
    "\n",
    "print(\"The average is: \", average(statsXGB))\n",
    "print(\"The median is: \", median(statsXGB))\n",
    "\n",
    "\n",
    "# Measure the runtime of a simulation\n",
    "\n",
    "runTime = endTime - startTime\n",
    "print(\"Duration: \", runTime, \"s\")\n",
    "\n",
    "#Coefficient Plots\n",
    "dfLR = pd.concat(dataLR, ignore_index=True)\n",
    "dfLR.loc['mean'] = dfLR.mean()\n",
    "\n",
    "dfRF = pd.concat(dataRF, ignore_index=True)\n",
    "dfRF.loc['mean'] = dfRF.mean()\n",
    "\n",
    "dfSVC = pd.concat(dataSVC, ignore_index=True)\n",
    "dfSVC.loc['mean'] = dfSVC.mean()\n",
    "\n",
    "#Comment out below if using tree\n",
    "dfXGB = pd.concat(dataXGB, ignore_index=True)\n",
    "dfXGB.loc['mean'] = dfXGB.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Coefficient Data\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Logistic Regression Coefficient Weights - Standardized')\n",
    "bar_plot = plt.bar(range(len(dfLR.columns)), dfLR.loc['mean'], color='steelblue')\n",
    "plt.xticks(range(len(dfLR.columns)), dfLR.columns, fontsize=10)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "plt.savefig('LRCoefS.png')    \n",
    "plt.show()\n",
    "display(dfLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c794d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above Ordered and ABS\n",
    "\n",
    "dfLR2 = dfLR.loc['mean'].to_frame()\n",
    "dfLR2 = abs(dfLR2)\n",
    "dfLR2 = dfLR2.sort_values(by = ['mean'], ascending=False).transpose().reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Logistic Regression Coefficient Weights', fontsize = 18, pad=15)\n",
    "bar_plot = plt.bar(range(len(dfLR2.columns)), dfLR2.loc[0], color='steelblue')\n",
    "plt.xticks(range(len(dfLR2.columns)), dfLR2.columns, fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(fontsize=16)\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=16)\n",
    "plt.savefig('LRCoefS2.png', bbox_inches = 'tight')    \n",
    "plt.show()\n",
    "display(dfLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10467894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Random Forest Coefficients \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Random Forest Feature Importance - Standardized')\n",
    "bar_plot = plt.bar(range(len(dfRF.columns)), dfRF.loc['mean'], color='steelblue')\n",
    "plt.xticks(range(len(dfRF.columns)), dfRF.columns, fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(fontsize=12)\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "plt.savefig('RFCoefS.png')    \n",
    "plt.show()\n",
    "display(dfRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713260c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordered\n",
    "\n",
    "dfRF2 = dfRF.loc['mean'].to_frame()\n",
    "dfRF2 = dfRF2.sort_values(by = ['mean'], ascending=False).transpose().reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Random Forest Feature Importance', fontsize=18, pad=15)\n",
    "bar_plot = plt.bar(range(len(dfRF2.columns)), dfRF2.loc[0], color='steelblue')\n",
    "plt.xticks(range(len(dfRF2.columns)), dfRF2.columns, fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(fontsize=16)\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=16)\n",
    "plt.savefig('RFCoefS2.png', bbox_inches = 'tight')    \n",
    "plt.show()\n",
    "display(dfRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc272b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing SVC Coefficients \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Support Vector Classifier Coefficient Weights - Standardized')\n",
    "bar_plot = plt.bar(range(len(dfSVC.columns)), dfSVC.loc['mean'], color='steelblue')\n",
    "plt.xticks(range(len(dfSVC.columns)), dfSVC.columns, fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(fontsize=12)\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=10)   \n",
    "plt.savefig('SVCCoefS.png')  \n",
    "plt.show()\n",
    "display(dfSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordered and ABS\n",
    "\n",
    "dfSVC2 = dfSVC.loc['mean'].to_frame()\n",
    "dfSVC2 = abs(dfSVC2)\n",
    "dfSVC2 = dfSVC2.sort_values(by = ['mean'], ascending=False).transpose().reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('SVC Coefficient Weights', fontsize = 18, pad = 15)\n",
    "bar_plot = plt.bar(range(len(dfSVC2.columns)), dfSVC2.loc[0], color='steelblue')\n",
    "plt.xticks(range(len(dfSVC2.columns)), dfSVC2.columns, fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(fontsize=16)\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=16) \n",
    "plt.savefig('SVCCoefS2.jpg', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "display(dfSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbba4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing XGB Coefficients \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('eXtreme Gradient Boost Coefficient Weights - Standardized')\n",
    "bar_plot = plt.bar(range(len(dfXGB.columns)), dfXGB.loc['mean'], color='steelblue')\n",
    "plt.xticks(range(len(dfXGB.columns)), dfXGB.columns, fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(fontsize=12)\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=10)   \n",
    "plt.savefig('XGBCoefS.png')  \n",
    "plt.show()\n",
    "display(dfXGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordered and ABS\n",
    "\n",
    "dfXGB2 = dfXGB.loc['mean'].to_frame()\n",
    "dfXGB2 = abs(dfXGB2)\n",
    "dfXGB2 = dfXGB2.sort_values(by = ['mean'], ascending=False).transpose().reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('XGB Coefficient Weights', fontsize = 18, pad = 15)\n",
    "bar_plot = plt.bar(range(len(dfXGB2.columns)), dfXGB2.loc[0], color='steelblue')\n",
    "plt.xticks(range(len(dfXGB2.columns)), dfXGB2.columns, fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(fontsize=16)\n",
    "for rect in bar_plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=16) \n",
    "plt.savefig('XGBCoefS2.jpg', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "display(dfXGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e686e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Average Rank of Features: \n",
    "\n",
    "rank_list = [dfLR2, dfRF2, dfSVC2] #list of coefficient frames\n",
    "for df in rank_list:\n",
    "    df.loc[len(df.index)] = list(range(1,15)) # adding row in df representing ranking\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining rankings across models and then displaying average ranking\n",
    "\n",
    "for feature in X.columns.to_list():\n",
    "    total_rank = 0;\n",
    "    for df in rank_list: \n",
    "        total_rank += df.iloc[1][feature]\n",
    "    total_rank = total_rank / len(rank_list)\n",
    "    print(feature ,\": \", total_rank)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9aafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(hs[['FSL','HW','Rin','fAHP','Peak','Rheo','Thresh','SA_45','multi']], hue='multi', palette=\"bright\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation\n",
    "\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "sns.heatmap(hs.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True, annot_kws={\"fontsize\":13})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02bc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PCA Analysis\n",
    "\n",
    "# model = PCA(n_components=2)              # 2. Instantiate the model with hyperparameters\n",
    "# model.fit(X)                        # 3. Fit to data. Notice that y is not specified!\n",
    "# X_2D = model.transform(X)\n",
    "\n",
    "# hs['PCA1'] = X_2D[:, 0]\n",
    "# hs['PCA2'] = X_2D[:, 1]\n",
    "# sns.set(font_scale=1)\n",
    "# sns.lmplot(x = \"PCA1\", y = \"PCA2\", hue='multi', data=hs, fit_reg=False);\n",
    "\n",
    "# # Assuming 'model' is your trained PCA model\n",
    "\n",
    "# # Get the loadings (coefficients) for each feature and each principal component\n",
    "# loadings = model.components_\n",
    "\n",
    "# # Get feature names\n",
    "# feature_names = X.columns\n",
    "\n",
    "# # Displaying loadings for each feature and each principal component\n",
    "# for pc_idx, loading in enumerate(loadings):\n",
    "#     print(f\"Principal Component {pc_idx + 1} Loadings:\")\n",
    "#     for feature_name, weight in zip(feature_names, loading):\n",
    "#         print(f\"{feature_name}: {weight}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "statsLR = []\n",
    "statsKNN = []\n",
    "statsRF = []\n",
    "statsSVC = []\n",
    "stastXGB = []\n",
    "num_simulations = 1000\n",
    "startTime = 0 \n",
    "endTime = 0\n",
    "dataLR = []\n",
    "dataRF = []\n",
    "dataSVC = []\n",
    "dataXGB = []\n",
    "acc = []\n",
    "outlst = []\n",
    "df = pd.DataFrame(columns = X_train.columns)\n",
    "outliers = pd.DataFrame(columns = X_test.columns)\n",
    "\n",
    "\n",
    "# Loop through simulations\n",
    "startTime = time.perf_counter()\n",
    "for i in range(num_simulations):\n",
    "    statsLR.append(logisticRegression(dataLR))\n",
    "    \n",
    "for i in range(num_simulations):\n",
    "    statsKNN.append(KNN())\n",
    "    \n",
    "for i in range(num_simulations):\n",
    "    statsRF.append(randomForest(dataRF))\n",
    "    \n",
    "for i in range(num_simulations):\n",
    "    statsSVC.append(supportVectorClassifier(dataSVC))\n",
    "    \n",
    "for i in range(num_simulations):\n",
    "    statsXGB.append(xgBoost(dataXGB))\n",
    "    \n",
    "endTime = time.perf_counter()\n",
    "    \n",
    "#sns.histplot(data=stats, binwidth=0.5)\n",
    "\n",
    "models = ['Logistic\\nRegression', 'K Nearest\\nNeighbors', 'Random\\nForest',\n",
    "         'Support\\nVector Classifier', 'eXtreme\\nGradient Boost']\n",
    "stat_lst = [statsLR, statsKNN, statsRF, statsSVC, statsXGB]\n",
    "statdf = pd.DataFrame(stat_lst) \n",
    "statdf = statdf.transpose()\n",
    "statdf.columns = models\n",
    "print(statdf)\n",
    "\n",
    "# Iterate through the five models\n",
    "for i in range(len(models)):\n",
    "    # Draw the density plot\n",
    "    sns.violinplot(data=statdf, inner='box', cut=0)\n",
    "    \n",
    "# Plotting Modifications\n",
    "# plt.legend(prop={'size': 8}, title = 'Model')\n",
    "plt.title('Violin Plot with Multiple Models - Standardized')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('MCViolinS.jpg')\n",
    "\n",
    "# Provide metrics for the simulations\n",
    "\n",
    "print(\"The LR average is: \", average(statsLR))\n",
    "print(\"The LR SEM is:\", sem(statsLR))\n",
    "print(\"The LR median is: \", median(statsLR))\n",
    "print(\"The LR range is: \", rangefunc(statsLR))\n",
    "\n",
    "print(\"The KNN average is: \", average(statsKNN))\n",
    "print(\"The KNN SEM is:\", sem(statsKNN))\n",
    "print(\"The KNN median is: \", median(statsKNN))\n",
    "print(\"The KNN range is: \", rangefunc(statsKNN))\n",
    "\n",
    "print(\"The RF average is: \", average(statsRF))\n",
    "print(\"The RF SEM is:\", sem(statsRF))\n",
    "print(\"The RF median is: \", median(statsRF))\n",
    "print(\"The RF range is: \", rangefunc(statsRF))\n",
    "\n",
    "print(\"The SVC average is: \", average(statsSVC))\n",
    "print(\"The SVC SEM is:\", sem(statsSVC))\n",
    "print(\"The SVC median is: \", median(statsSVC))\n",
    "print(\"The SVC range is: \", rangefunc(statsSVC))\n",
    "\n",
    "print(\"The XGB average is: \", average(statsXGB))\n",
    "print(\"The XGB SEM is:\", sem(statsXGB))\n",
    "print(\"The XGB median is: \", median(statsXGB))\n",
    "print(\"The XGB range is: \", rangefunc(statsXGB))\n",
    "\n",
    "# Measure the runtime of a simulation\n",
    "\n",
    "runTime = endTime - startTime\n",
    "print(\"Duration: \", runTime, \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = aw.Document()\n",
    "builder = aw.DocumentBuilder(doc)\n",
    "\n",
    "shape = builder.insert_image(\"MCViolinS.jpg\")\n",
    "shape.get_shape_renderer().save(\"OutputS.tiff\", aw.saving.ImageSaveOptions(aw.SaveFormat.TIFF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fd797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
